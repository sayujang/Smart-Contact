# -*- coding: utf-8 -*-
"""vits_train_1-3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rUZGbhWSI6DqGwo55-tawIMiaWZUQD1n
"""

!pip install datasets torchcodec soundfile --quiet

import os
from datasets import load_dataset, Audio

# Load dataset
ds = load_dataset("gauravparajuli/slr43")

# Decode audio properly
ds = ds.cast_column("audio", Audio(sampling_rate=22050))

# Folder to save
out_dir = "/content/slr43_data"
os.makedirs(out_dir, exist_ok=True)

import soundfile as sf
import shutil

for idx, example in enumerate(ds["train"]):
    audio = example["audio"]
    transcript = example["text"]

    arr = audio["array"]
    sr = audio["sampling_rate"]

    fname = f"utt_{idx:04d}.wav"
    path = os.path.join(out_dir, fname)

    sf.write(path, arr, sr)
    with open(path + ".txt", "w", encoding="utf-8") as f:
        f.write(transcript)

    if (idx + 1) % 100 == 0:
        print(f"‚úÖ Saved {idx + 1} audio files")

print("All done!")

# play 10 random sampels
import os, random
from IPython.display import Audio, display

# Directory containing your .wav and .txt files
data_dir = "/content/slr43_data"

# Get all wav files
wav_files = sorted([f for f in os.listdir(data_dir) if f.endswith(".wav")])

# Randomly select 10 samples
samples = random.sample(wav_files, 10)

for fname in samples:
    wav_path = os.path.join(data_dir, fname)
    txt_path = wav_path + ".txt"

    # Display audio player
    display(Audio(filename=wav_path))

    # Display transcription text (if exists)
    if os.path.exists(txt_path):
        with open(txt_path, "r", encoding="utf-8") as f:
            text = f.read().strip()
        print(f"üó£Ô∏è Transcription ({fname}):\n{text}\n{'-'*60}\n")
    else:
        print(f"‚ö†Ô∏è No transcription found for {fname}\n{'-'*60}\n")

# calculate total duration

import os
import soundfile as sf

data_dir = "/content/slr43_data"

total_duration = 0.0  # in seconds
count = 0

for fname in os.listdir(data_dir):
    if fname.endswith(".wav"):
        file_path = os.path.join(data_dir, fname)
        with sf.SoundFile(file_path) as audio:
            frames = len(audio)
            sr = audio.samplerate
            duration = frames / sr
            total_duration += duration
            count += 1

hours = total_duration / 3600
print(f"üïí Processed {count} audio files.")
print(f"üéß Total duration: {total_duration:.2f} seconds ({hours:.2f} hours)")

# Commented out IPython magic to ensure Python compatibility.
# preparing the dataset for training vits

# GPU check
!nvidia-smi

# Install espeak (for phonemizer)
!apt-get install -y espeak

# Install updated Python packages
!pip install Cython librosa matplotlib numpy phonemizer scipy tensorboard torch torchvision Unidecode

# Clone VITS repo
!git clone https://github.com/jaywalnut310/vits.git
# %cd vits

import os
import glob
import random
import shutil

# Paths
source_dir = "/content/slr43_data"  # original dataset folder
dataset_root = "/content/nepali_tts_dataset"
train_dir = os.path.join(dataset_root, "train")
val_dir = os.path.join(dataset_root, "val")
filelists_dir = os.path.join(dataset_root, "filelists")

os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)
os.makedirs(filelists_dir, exist_ok=True)

# Collect all wav files
wav_files = glob.glob(os.path.join(source_dir, "*.wav"))
wav_files.sort()
random.shuffle(wav_files)

# Train/validation split (90/10)
split_idx = int(len(wav_files) * 0.9)
train_files = wav_files[:split_idx]
val_files = wav_files[split_idx:]

def copy_and_create_txt(files, target_dir, filelist_path):
    with open(filelist_path, "w", encoding="utf-8") as f:
        for idx, wav_path in enumerate(files):
            # Define standardized filename
            filename = f"{idx:04d}.wav"
            new_wav_path = os.path.join(target_dir, filename)

            # Copy wav
            shutil.copy(wav_path, new_wav_path)

            # Create txt file with same name
            transcript_path = os.path.splitext(wav_path)[0] +".wav"+".txt"
            if os.path.exists(transcript_path):
                with open(transcript_path, "r", encoding="utf-8") as tf:
                    text = tf.read().strip()
            else:
                text = "dummy text"  # if no transcript, you must replace it later

            new_txt_path = os.path.join(target_dir, f"{idx:04d}.txt")
            with open(new_txt_path, "w", encoding="utf-8") as tf:
                tf.write(text)

            # Write filelist entry
            f.write(f"{new_wav_path}|{text}\n")

# Create train and val sets
copy_and_create_txt(train_files, train_dir, os.path.join(filelists_dir, "train_filelist.txt"))
copy_and_create_txt(val_files, val_dir, os.path.join(filelists_dir, "val_filelist.txt"))

print("Dataset preparation complete!")

!head /content/nepali_tts_dataset/filelists/train_filelist.txt

""" Nepali Devanagari symbols for VITS TTS
Defines the set of symbols used in text input to the model for Nepali language.
"""

# Special tokens
_pad = '_'

# Punctuation - including Devanagari punctuation marks
_punctuation = '‡•§‡••,;:!?.‚Äì‚Äî‚Ä¶"\'()[]{}/'  # ‡•§ is Devanagari full stop, ‡•• is double danda

# Independent vowels (‡§∏‡•ç‡§µ‡§∞) - Nepali uses 12 main vowels
_vowels = '‡§Ö‡§Ü‡§á‡§à‡§â‡§ä‡§ã‡§è‡§ê‡§ì‡§î‡§Ö‡§Ç‡§Ö‡§É'
# Extended vowels (less common, but present in Sanskrit loanwords)
_vowels_extended = '‡•†‡§å‡•°'

# Consonants (‡§µ‡•ç‡§Ø‡§û‡•ç‡§ú‡§®) - 36 main consonants in Nepali
# Organized by articulation groups
_velars = '‡§ï‡§ñ‡§ó‡§ò‡§ô'           # ka, kha, ga, gha, nga
_palatals = '‡§ö‡§õ‡§ú‡§ù‡§û'         # cha, chha, ja, jha, nya
_retroflexes = '‡§ü‡§†‡§°‡§¢‡§£'      # ·π≠a, ·π≠ha, ·∏ça, ·∏çha, ·πáa
_dentals = '‡§§‡§•‡§¶‡§ß‡§®'          # ta, tha, da, dha, na
_labials = '‡§™‡§´‡§¨‡§≠‡§Æ'          # pa, pha, ba, bha, ma
_semivowels = '‡§Ø‡§∞‡§≤‡§µ'        # ya, ra, la, va/wa
_sibilants = '‡§∂‡§∑‡§∏'          # ≈õa, ·π£a, sa
_glottal = '‡§π'              # ha

# Additional Nepali-specific consonants (used but not in traditional Sanskrit)
_additional_consonants = '‡§°‡§º‡§¢‡§º'  # ·πõa (flapped ‡§∞), ·πõha (often used in borrowed words)

# Conjunct consonants treated as separate letters in Nepali (placed at end of alphabet)
_conjuncts_as_letters = '‡§ï‡•ç‡§∑‡§§‡•ç‡§∞‡§ú‡•ç‡§û'  # k·π£a, tra, j√±a (treated as 3 additional consonants)

# Combine all consonants
_consonants = _velars + _palatals + _retroflexes + _dentals + _labials + _semivowels + _sibilants + _glottal + _additional_consonants

# Dependent vowel signs/matras (‡§Ü‡§∂‡•ç‡§∞‡§ø‡§§ ‡§∏‡•ç‡§µ‡§∞ / ‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ)
# These modify consonants
_dependent_vowels = '‡§æ‡§ø‡•Ä‡•Å‡•Ç‡•É‡•Ñ‡•á‡•à‡•ã‡•å'  # ƒÅ, i, ƒ´, u, ≈´, ·πõ, ·πù, e, ai, o, au
# Note: '‡§Ö' (a) has no matra as it's inherent in consonants

# Vowel modifiers
_chandrabindu = '‡§Å'    # chandrabindu (nasalization marker)
_anusvara = '‡§Ç'        # anusvara (nasal marker, often used instead of chandrabindu)
_visarga = '‡§É'         # visarga (used in Sanskrit loanwords)

# Virama/Halant - removes inherent vowel from consonant
_virama = '‡•ç'          # halant/virama (‡•ç)

# Devanagari numerals (0-9 in Devanagari)
_numbers = '‡•¶‡•ß‡•®‡•©‡•™‡•´‡•¨‡•≠‡•Æ‡•Ø'

# Additional signs
_om = '‡•ê'             # Om symbol (religious/cultural significance)
_avagraha = '‡§Ω'        # avagraha (rarely used)
_nukta = '‡§º'           # nukta (used to create ‡§°‡§º and ‡§¢‡§º)

# Zero-width characters (important for proper rendering)
_zwj = '\u200D'        # Zero Width Joiner (for half-forms)
_zwnj = '\u200C'       # Zero Width Non-Joiner (prevents conjuncts)

# Space
_space = ' '

# Combine all symbols
# Core symbols needed for Nepali TTS (grapheme-level)
symbols = (
    [_pad] +
    list(_punctuation) +
    list(_vowels) +
    list(_vowels_extended) +
    list(_consonants) +
    list(_conjuncts_as_letters) +
    list(_dependent_vowels) +
    [_chandrabindu, _anusvara, _visarga] +
    [_virama] +
    list(_numbers) +
    [_space]
)

# Optional: Extended symbols (uncomment if needed for your dataset)
# symbols += [_om, _avagraha, _nukta, _zwj, _zwnj]

# Special symbol ids
SPACE_ID = symbols.index(" ")

# Symbol to ID mapping
_symbol_to_id = {s: i for i, s in enumerate(symbols)}
_id_to_symbol = {i: s for i, s in enumerate(symbols)}

# Utility functions
def text_to_sequence(text):
    """Converts a string of text to a sequence of IDs"""
    return [_symbol_to_id[symbol] for symbol in text if symbol in _symbol_to_id]

def sequence_to_text(sequence):
    """Converts a sequence of IDs to a string of text"""
    return ''.join([_id_to_symbol[symbol_id] for symbol_id in sequence if symbol_id in _id_to_symbol])

def clean_text(text):
    """Basic text cleaning for Nepali"""
    # Normalize to NFD (some Nepali text might use combined characters)
    import unicodedata
    text = unicodedata.normalize('NFD', text)
    # Filter out characters not in our symbol set
    text = ''.join([char for char in text if char in _symbol_to_id or char == ' '])
    return text

# Print statistics (useful for debugging)
if __name__ == "__main__":
    print(f"Total symbols: {len(symbols)}")
    print(f"Vowels: {len(_vowels) + len(_vowels_extended)}")
    print(f"Consonants: {len(_consonants) + len(_conjuncts_as_letters)}")
    print(f"Dependent vowels (matras): {len(_dependent_vowels)}")
    print(f"Numbers: {len(_numbers)}")
    print(f"\nAll symbols:")
    print(''.join(symbols))
    print(f"\nSymbol list: {symbols}")

    # Test examples
    print("\n--- Test Examples ---")
    test_texts = [
        "‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞",
        "‡§Ø‡•ã ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§π‡•ã‡•§",
        "‡§Æ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§¨‡•ã‡§≤‡•ç‡§õ‡•Å‡•§",
        "‡§ï‡•ç‡§∑‡§Æ‡§æ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç",
    ]

    for text in test_texts:
        try:
            seq = text_to_sequence(text)
            back = sequence_to_text(seq)
            print(f"Original: {text}")
            print(f"Sequence: {seq}")
            print(f"Reconstructed: {back}")
            print(f"Match: {text == back}\n")
        except Exception as e:
            print(f"Error processing '{text}': {e}\n")

_symbol_to_id

for s in _dependent_vowels:
    print(s, hex(ord(s)))

! cat /content/nepali_tts_dataset/train/0000.txt

text = "‡§ï‡§ø‡§∑‡•ç‡§ü ‡§¨‡•à‡§ô‡•ç‡§ï ‡§≤‡§ø‡§Æ‡§ø‡§ü‡•á‡§° ‡§™‡•ç‡§∞‡§≠‡•Å ‡§µ‡§ø‡§ï‡§æ‡§∏ ‡§¨‡•à‡§ô‡•ç‡§ï ‡§≤‡§ø‡§Æ‡§ø‡§ü‡•á‡§° ‡§ó‡•å‡§∞‡•Ä‡§∂‡§Ç‡§ï‡§∞ ‡§°‡•á‡§≠‡§≤‡§™‡§Æ‡•á‡§£‡•ç‡§ü ‡§¨‡•à‡§ô‡•ç‡§ï ‡§≤‡§ø‡§Æ‡§ø‡§ü‡•á‡§°  ‡§∞ ‡§ú‡•á‡§®‡§ø‡§• ‡§´‡§æ‡§á‡§®‡§æ‡§®‡•ç‡§∏ ‡§≤‡§ø‡§Æ‡§ø‡§ü‡•á‡§° ‡§ó‡§æ‡§≠‡§ø‡§è ‡•§"

seq = text_to_sequence(text)
back = sequence_to_text(seq)
print(f"Original: {text}")
print(f"Sequence: {seq}")
print(f"Reconstructed: {back}")
print(f"Match: {text == back}\n")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/vits/text/cleaners.py
# # (PASTE your full modified cleaners.py content here)
# """ from https://github.com/keithito/tacotron """
# 
# '''
# Cleaners are transformations that run over the input text at both training and eval time.
# 
# Cleaners can be selected by passing a comma-delimited list of cleaner names as the "cleaners"
# hyperparameter. Some cleaners are English-specific. You'll typically want to use:
#   1. "english_cleaners" for English text
#   2. "transliteration_cleaners" for non-English text that can be transliterated to ASCII using
#      the Unidecode library (https://pypi.python.org/pypi/Unidecode)
#   3. "basic_cleaners" if you do not want to transliterate (in this case, you should also update
#      the symbols in symbols.py to match your data).
# '''
# 
# import re
# from unidecode import unidecode
# from phonemizer import phonemize
# from text.symbols import symbols
# 
# _symbol_to_id = {s: i for i, s in enumerate(symbols)}
# 
# 
# # Regular expression matching whitespace:
# _whitespace_re = re.compile(r'\s+')
# 
# # List of (regular expression, replacement) pairs for abbreviations:
# _abbreviations = [(re.compile('\\b%s\\.' % x[0], re.IGNORECASE), x[1]) for x in [
#   ('mrs', 'misess'),
#   ('mr', 'mister'),
#   ('dr', 'doctor'),
#   ('st', 'saint'),
#   ('co', 'company'),
#   ('jr', 'junior'),
#   ('maj', 'major'),
#   ('gen', 'general'),
#   ('drs', 'doctors'),
#   ('rev', 'reverend'),
#   ('lt', 'lieutenant'),
#   ('hon', 'honorable'),
#   ('sgt', 'sergeant'),
#   ('capt', 'captain'),
#   ('esq', 'esquire'),
#   ('ltd', 'limited'),
#   ('col', 'colonel'),
#   ('ft', 'fort'),
# ]]
# 
# 
# def expand_abbreviations(text):
#   for regex, replacement in _abbreviations:
#     text = re.sub(regex, replacement, text)
#   return text
# 
# 
# def expand_numbers(text):
#   return normalize_numbers(text)
# 
# 
# def lowercase(text):
#   return text.lower()
# 
# 
# def collapse_whitespace(text):
#   return re.sub(_whitespace_re, ' ', text)
# 
# 
# def convert_to_ascii(text):
#   return unidecode(text)
# 
# 
# def basic_cleaners(text):
#   '''Basic pipeline that lowercases and collapses whitespace without transliteration.'''
#   text = lowercase(text)
#   text = collapse_whitespace(text)
#   return text
# 
# 
# def transliteration_cleaners(text):
#   '''Pipeline for non-English text that transliterates to ASCII.'''
#   text = convert_to_ascii(text)
#   text = lowercase(text)
#   text = collapse_whitespace(text)
#   return text
# 
# 
# def english_cleaners(text):
#   '''Pipeline for English text, including abbreviation expansion.'''
#   text = convert_to_ascii(text)
#   text = lowercase(text)
#   text = expand_abbreviations(text)
#   phonemes = phonemize(text, language='en-us', backend='espeak', strip=True)
#   phonemes = collapse_whitespace(phonemes)
#   return phonemes
# 
# def nepali_cleaners(text):
#     """Basic text cleaning for Nepali"""
#     # Normalize to NFD (some Nepali text might use combined characters)
#     import unicodedata
#     text = unicodedata.normalize('NFD', text)
#     # Filter out characters not in our symbol set
#     text = ''.join([char for char in text if char in _symbol_to_id or char == ' '])
#     return text
# 
# def english_cleaners2(text):
#   '''Pipeline for English text, including abbreviation expansion. + punctuation + stress'''
#   text = convert_to_ascii(text)
#   text = lowercase(text)
#   text = expand_abbreviations(text)
#   phonemes = phonemize(text, language='en-us', backend='espeak', strip=True, preserve_punctuation=True, with_stress=True)
#   phonemes = collapse_whitespace(phonemes)
#   return phonemes
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/vits/text/__init__.py
# # (PASTE your full modified __init__.py content here)
# """ from https://github.com/keithito/tacotron """
# from text import cleaners
# from text.symbols import symbols
# 
# 
# # # Mappings from symbol to numeric ID and vice versa:
# # _symbol_to_id = {s: i for i, s in enumerate(symbols)}
# # _id_to_symbol = {i: s for i, s in enumerate(symbols)}
# 
# 
# # def text_to_sequence(text, cleaner_names):
# #   '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.
# #     Args:
# #       text: string to convert to a sequence
# #       cleaner_names: names of the cleaner functions to run the text through
# #     Returns:
# #       List of integers corresponding to the symbols in the text
# #   '''
# #   sequence = []
# 
# #   clean_text = _clean_text(text, cleaner_names)
# #   for symbol in clean_text:
# #     symbol_id = _symbol_to_id[symbol]
# #     sequence += [symbol_id]
# #   return sequence
# 
# 
# # def cleaned_text_to_sequence(cleaned_text):
# #   '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.
# #     Args:
# #       text: string to convert to a sequence
# #     Returns:
# #       List of integers corresponding to the symbols in the text
# #   '''
# #   sequence = [_symbol_to_id[symbol] for symbol in cleaned_text]
# #   return sequence
# 
# 
# # def sequence_to_text(sequence):
# #   '''Converts a sequence of IDs back to a string'''
# #   result = ''
# #   for symbol_id in sequence:
# #     s = _id_to_symbol[symbol_id]
# #     result += s
# #   return result
# 
# 
# # def _clean_text(text, cleaner_names):
# #   for name in cleaner_names:
# #     cleaner = getattr(cleaners, name)
# #     if not cleaner:
# #       raise Exception('Unknown cleaner: %s' % name)
# #     text = cleaner(text)
# #   return text
# 
# 
#  #Symbol to ID mapping
# _symbol_to_id = {s: i for i, s in enumerate(symbols)}
# _id_to_symbol = {i: s for i, s in enumerate(symbols)}
# 
# # Utility functions
# def text_to_sequence(text):
#     """Converts a string of text to a sequence of IDs"""
#     return [_symbol_to_id[symbol] for symbol in text if symbol in _symbol_to_id]
# 
# def sequence_to_text(sequence):
#     """Converts a sequence of IDs to a string of text"""
#     return ''.join([_id_to_symbol[symbol_id] for symbol_id in sequence if symbol_id in _id_to_symbol])
# 
# def _clean_text(text, cleaner_names):
#   for name in cleaner_names:
#     cleaner = getattr(cleaners, name)
#     if not cleaner:
#       raise Exception('Unknown cleaner: %s' % name)
#     text = cleaner(text)
#   return text
# 
# def cleaned_text_to_sequence(cleaned_text):
#     """Converts a string of text to a sequence of IDs"""
#     return [_symbol_to_id[symbol] for symbol in cleaned_text if symbol in _symbol_to_id]

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/vits/text/symbols.py
# """ from https://github.com/keithito/tacotron """
# 
# '''
# Defines the set of symbols used in text input to the model.
# '''
# # _pad        = '_'
# # _punctuation = ';:,.!?¬°¬ø‚Äî‚Ä¶"¬´¬ª‚Äú‚Äù '
# # _letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
# # _letters_ipa = "…ë…ê…í√¶…ì ôŒ≤…î…ï√ß…ó…ñ√∞ §…ô…ò…ö…õ…ú…ù…û…ü Ñ…°…†…¢ õ…¶…ßƒß…• ú…®…™ ù…≠…¨…´…Æ ü…±…Ø…∞≈ã…≥…≤…¥√∏…µ…∏Œ∏≈ì…∂ ò…π…∫…æ…ª Ä Å…Ω Ç É à ß â ä ã‚±± å…£…§ çœá é è ë ê í î ° ï ¢«Ä«Å«Ç«ÉÀàÀåÀêÀë º ¥ ∞ ± ≤ ∑À†À§Àû‚Üì‚Üë‚Üí‚Üó‚Üò'Ã©'·µª"
# 
# 
# # # Export all symbols:
# # symbols = [_pad] + list(_punctuation) + list(_letters) + list(_letters_ipa)
# 
# # # Special symbol ids
# # SPACE_ID = symbols.index(" ")
# """ Nepali Devanagari symbols for VITS TTS
# Defines the set of symbols used in text input to the model for Nepali language.
# """
# 
# # Special tokens
# _pad = '_'
# 
# # Punctuation - including Devanagari punctuation marks
# _punctuation = '‡•§‡••,;:!?.‚Äì‚Äî‚Ä¶"\'()[]{}/'  # ‡•§ is Devanagari full stop, ‡•• is double danda
# 
# # Independent vowels (‡§∏‡•ç‡§µ‡§∞) - Nepali uses 12 main vowels
# _vowels = '‡§Ö‡§Ü‡§á‡§à‡§â‡§ä‡§ã‡§è‡§ê‡§ì‡§î‡§Ö‡§Ç‡§Ö‡§É'
# # Extended vowels (less common, but present in Sanskrit loanwords)
# _vowels_extended = '‡•†‡§å‡•°'
# 
# # Consonants (‡§µ‡•ç‡§Ø‡§û‡•ç‡§ú‡§®) - 36 main consonants in Nepali
# # Organized by articulation groups
# _velars = '‡§ï‡§ñ‡§ó‡§ò‡§ô'           # ka, kha, ga, gha, nga
# _palatals = '‡§ö‡§õ‡§ú‡§ù‡§û'         # cha, chha, ja, jha, nya
# _retroflexes = '‡§ü‡§†‡§°‡§¢‡§£'      # ·π≠a, ·π≠ha, ·∏ça, ·∏çha, ·πáa
# _dentals = '‡§§‡§•‡§¶‡§ß‡§®'          # ta, tha, da, dha, na
# _labials = '‡§™‡§´‡§¨‡§≠‡§Æ'          # pa, pha, ba, bha, ma
# _semivowels = '‡§Ø‡§∞‡§≤‡§µ'        # ya, ra, la, va/wa
# _sibilants = '‡§∂‡§∑‡§∏'          # ≈õa, ·π£a, sa
# _glottal = '‡§π'              # ha
# 
# # Additional Nepali-specific consonants (used but not in traditional Sanskrit)
# _additional_consonants = '‡§°‡§º‡§¢‡§º'  # ·πõa (flapped ‡§∞), ·πõha (often used in borrowed words)
# 
# # Conjunct consonants treated as separate letters in Nepali (placed at end of alphabet)
# _conjuncts_as_letters = '‡§ï‡•ç‡§∑‡§§‡•ç‡§∞‡§ú‡•ç‡§û'  # k·π£a, tra, j√±a (treated as 3 additional consonants)
# 
# # Combine all consonants
# _consonants = _velars + _palatals + _retroflexes + _dentals + _labials + _semivowels + _sibilants + _glottal + _additional_consonants
# 
# # Dependent vowel signs/matras (‡§Ü‡§∂‡•ç‡§∞‡§ø‡§§ ‡§∏‡•ç‡§µ‡§∞ / ‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ)
# # These modify consonants
# _dependent_vowels = '‡§æ‡§ø‡•Ä‡•Å‡•Ç‡•É‡•Ñ‡•á‡•à‡•ã‡•å'  # ƒÅ, i, ƒ´, u, ≈´, ·πõ, ·πù, e, ai, o, au
# # Note: '‡§Ö' (a) has no matra as it's inherent in consonants
# 
# # Vowel modifiers
# _chandrabindu = '‡§Å'    # chandrabindu (nasalization marker)
# _anusvara = '‡§Ç'        # anusvara (nasal marker, often used instead of chandrabindu)
# _visarga = '‡§É'         # visarga (used in Sanskrit loanwords)
# 
# # Virama/Halant - removes inherent vowel from consonant
# _virama = '‡•ç'          # halant/virama (‡•ç)
# 
# # Devanagari numerals (0-9 in Devanagari)
# _numbers = '‡•¶‡•ß‡•®‡•©‡•™‡•´‡•¨‡•≠‡•Æ‡•Ø'
# 
# # Additional signs
# _om = '‡•ê'             # Om symbol (religious/cultural significance)
# _avagraha = '‡§Ω'        # avagraha (rarely used)
# _nukta = '‡§º'           # nukta (used to create ‡§°‡§º and ‡§¢‡§º)
# 
# # Zero-width characters (important for proper rendering)
# _zwj = '\u200D'        # Zero Width Joiner (for half-forms)
# _zwnj = '\u200C'       # Zero Width Non-Joiner (prevents conjuncts)
# 
# # Space
# _space = ' '
# 
# # Combine all symbols
# # Core symbols needed for Nepali TTS (grapheme-level)
# symbols = (
#     [_pad] +
#     list(_punctuation) +
#     list(_vowels) +
#     list(_vowels_extended) +
#     list(_consonants) +
#     list(_conjuncts_as_letters) +
#     list(_dependent_vowels) +
#     [_chandrabindu, _anusvara, _visarga] +
#     [_virama] +
#     list(_numbers) +
#     [_space]
# )
# 
# # Optional: Extended symbols (uncomment if needed for your dataset)
# # symbols += [_om, _avagraha, _nukta, _zwj, _zwnj]
#

# Commented out IPython magic to ensure Python compatibility.
# 
# # OR manually fix it:
# %%writefile /content/vits/mel_processing.py
# import torch
# import torch.utils.data
# import numpy as np
# import librosa
# import librosa.util as librosa_util
# from scipy.io.wavfile import read
# 
# 
# MAX_WAV_VALUE = 32768.0
# 
# 
# def load_wav(full_path):
#     sampling_rate, data = read(full_path)
#     return data, sampling_rate
# 
# 
# def dynamic_range_compression(x, C=1, clip_val=1e-5):
#     return np.log(np.clip(x, a_min=clip_val, a_max=None) * C)
# 
# 
# def dynamic_range_decompression(x, C=1):
#     return np.exp(x) / C
# 
# 
# def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):
#     return torch.log(torch.clamp(x, min=clip_val) * C)
# 
# 
# def dynamic_range_decompression_torch(x, C=1):
#     return torch.exp(x) / C
# 
# 
# def spectral_normalize_torch(magnitudes):
#     output = dynamic_range_compression_torch(magnitudes)
#     return output
# 
# 
# def spectral_de_normalize_torch(magnitudes):
#     output = dynamic_range_decompression_torch(magnitudes)
#     return output
# 
# 
# mel_basis = {}
# hann_window = {}
# 
# 
# def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):
#     if torch.min(y) < -1.:
#         print('min value is ', torch.min(y))
#     if torch.max(y) > 1.:
#         print('max value is ', torch.max(y))
# 
#     global hann_window
#     dtype_device = str(y.dtype) + '_' + str(y.device)
#     wnsize_dtype_device = str(win_size) + '_' + dtype_device
#     if wnsize_dtype_device not in hann_window:
#         hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)
# 
#     y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')
#     y = y.squeeze(1)
# 
#     # in spectrogram_torch
#     spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size,
#                       window=hann_window[wnsize_dtype_device],
#                       center=center, pad_mode='reflect',
#                       normalized=False, onesided=True, return_complex=True)
#     spec = torch.abs(spec) + 1e-6
# 
#     # in mel_spectrogram_torch: same stft call, then
#     spec = torch.abs(spec) + 1e-6
#     return spec
# 
# 
# def spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):
#     global mel_basis
#     dtype_device = str(spec.dtype) + '_' + str(spec.device)
#     fmax_dtype_device = str(fmax) + '_' + dtype_device
#     if fmax_dtype_device not in mel_basis:
#         mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
#         mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)
#     spec = torch.matmul(mel_basis[fmax_dtype_device], spec)
#     spec = spectral_normalize_torch(spec)
#     return spec
# 
# 
# def mel_spectrogram_torch(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):
#     if torch.min(y) < -1.:
#         print('min value is ', torch.min(y))
#     if torch.max(y) > 1.:
#         print('max value is ', torch.max(y))
# 
#     global mel_basis, hann_window
#     dtype_device = str(y.dtype) + '_' + str(y.device)
#     fmax_dtype_device = str(fmax) + '_' + dtype_device
#     wnsize_dtype_device = str(win_size) + '_' + dtype_device
#     if fmax_dtype_device not in mel_basis:
#         mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)
#         mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=y.dtype, device=y.device)
#     if wnsize_dtype_device not in hann_window:
#         hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)
# 
#     y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')
#     y = y.squeeze(1)
# 
#     spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],
#                       center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=False)
# 
#     spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)
# 
#     spec = torch.matmul(mel_basis[fmax_dtype_device], spec)
#     spec = spectral_normalize_torch(spec)
# 
#     return spec
# 
# 
# def librosa_mel_fn(sr, n_fft, n_mels, fmin, fmax):
#     return librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/vits/train.py
# 
# import os
# import json
# import argparse
# import itertools
# import math
# import torch
# from torch import nn, optim
# from torch.nn import functional as F
# from torch.utils.data import DataLoader
# from torch.utils.tensorboard import SummaryWriter
# import torch.multiprocessing as mp
# import torch.distributed as dist
# from torch.nn.parallel import DistributedDataParallel as DDP
# from torch.cuda.amp import GradScaler
# from torch import amp
# 
# import commons
# import utils
# from data_utils import (
#   TextAudioLoader,
#   TextAudioCollate,
#   DistributedBucketSampler
# )
# from models import (
#   SynthesizerTrn,
#   MultiPeriodDiscriminator,
# )
# from losses import (
#   generator_loss,
#   discriminator_loss,
#   feature_loss,
#   kl_loss
# )
# from mel_processing import mel_spectrogram_torch, spec_to_mel_torch
# from text.symbols import symbols
# 
# 
# torch.backends.cudnn.benchmark = True
# global_step = 0
# 
# 
# def main():
#   """Assume Single Node Multi GPUs Training Only"""
#   assert torch.cuda.is_available(), "CPU training is not allowed."
# 
#   n_gpus = torch.cuda.device_count()
#   os.environ['MASTER_ADDR'] = 'localhost'
#   os.environ['MASTER_PORT'] = '80000'
# 
#   hps = utils.get_hparams()
#   mp.spawn(run, nprocs=n_gpus, args=(n_gpus, hps,))
# 
# 
# def run(rank, n_gpus, hps):
#   global global_step
#   if rank == 0:
#     logger = utils.get_logger(hps.model_dir)
#     logger.info(hps)
#     utils.check_git_hash(hps.model_dir)
#     writer = SummaryWriter(log_dir=hps.model_dir)
#     writer_eval = SummaryWriter(log_dir=os.path.join(hps.model_dir, "eval"))
# 
#   dist.init_process_group(backend='nccl', init_method='env://', world_size=n_gpus, rank=rank)
#   torch.manual_seed(hps.train.seed)
#   torch.cuda.set_device(rank)
# 
#   train_dataset = TextAudioLoader(hps.data.training_files, hps.data)
#   train_sampler = DistributedBucketSampler(
#       train_dataset,
#       hps.train.batch_size,
#       [32,300,400,500,600,700,800,900,1000],
#       num_replicas=n_gpus,
#       rank=rank,
#       shuffle=True)
#   collate_fn = TextAudioCollate()
#   train_loader = DataLoader(train_dataset, num_workers=8, shuffle=False, pin_memory=True,
#       collate_fn=collate_fn, batch_sampler=train_sampler)
#   if rank == 0:
#     eval_dataset = TextAudioLoader(hps.data.validation_files, hps.data)
#     eval_loader = DataLoader(eval_dataset, num_workers=8, shuffle=False,
#         batch_size=hps.train.batch_size, pin_memory=True,
#         drop_last=False, collate_fn=collate_fn)
# 
#   net_g = SynthesizerTrn(
#       len(symbols),
#       hps.data.filter_length // 2 + 1,
#       hps.train.segment_size // hps.data.hop_length,
#       **hps.model).cuda(rank)
#   net_d = MultiPeriodDiscriminator(hps.model.use_spectral_norm).cuda(rank)
#   optim_g = torch.optim.AdamW(
#       net_g.parameters(),
#       hps.train.learning_rate,
#       betas=hps.train.betas,
#       eps=hps.train.eps)
#   optim_d = torch.optim.AdamW(
#       net_d.parameters(),
#       hps.train.learning_rate,
#       betas=hps.train.betas,
#       eps=hps.train.eps)
#   net_g = DDP(net_g, device_ids=[rank])
#   net_d = DDP(net_d, device_ids=[rank])
# 
#   try:
#     _, _, _, epoch_str = utils.load_checkpoint(utils.latest_checkpoint_path(hps.model_dir, "G_*.pth"), net_g, optim_g)
#     _, _, _, epoch_str = utils.load_checkpoint(utils.latest_checkpoint_path(hps.model_dir, "D_*.pth"), net_d, optim_d)
#     global_step = (epoch_str - 1) * len(train_loader)
#   except:
#     epoch_str = 1
#     global_step = 0
# 
#   scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=hps.train.lr_decay, last_epoch=epoch_str-2)
#   scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=hps.train.lr_decay, last_epoch=epoch_str-2)
# 
#   scaler = GradScaler(enabled=hps.train.fp16_run)
# 
#   for epoch in range(epoch_str, hps.train.epochs + 1):
#     if rank==0:
#       train_and_evaluate(rank, epoch, hps, [net_g, net_d], [optim_g, optim_d], [scheduler_g, scheduler_d], scaler, [train_loader, eval_loader], logger, [writer, writer_eval])
#     else:
#       train_and_evaluate(rank, epoch, hps, [net_g, net_d], [optim_g, optim_d], [scheduler_g, scheduler_d], scaler, [train_loader, None], None, None)
#     scheduler_g.step()
#     scheduler_d.step()
# 
# 
# def train_and_evaluate(rank, epoch, hps, nets, optims, schedulers, scaler, loaders, logger, writers):
#   net_g, net_d = nets
#   optim_g, optim_d = optims
#   scheduler_g, scheduler_d = schedulers
#   train_loader, eval_loader = loaders
#   if writers is not None:
#     writer, writer_eval = writers
# 
#   train_loader.batch_sampler.set_epoch(epoch)
#   global global_step
# 
#   net_g.train()
#   net_d.train()
#   for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths) in enumerate(train_loader):
#     x, x_lengths = x.cuda(rank, non_blocking=True), x_lengths.cuda(rank, non_blocking=True)
#     spec, spec_lengths = spec.cuda(rank, non_blocking=True), spec_lengths.cuda(rank, non_blocking=True)
#     y, y_lengths = y.cuda(rank, non_blocking=True), y_lengths.cuda(rank, non_blocking=True)
# 
#     with amp.autocast('cuda', enabled=hps.train.fp16_run):
#       # debug
#       print("x", x.shape, "x_lengths", x_lengths, "spec", spec.shape, "spec_lengths", spec_lengths)
#       print("freq_bins_expected", hps.data.filter_length // 2 + 1)
#       y_hat, l_length, attn, ids_slice, x_mask, z_mask,\
#       (z, z_p, m_p, logs_p, m_q, logs_q) = net_g(x, x_lengths, spec, spec_lengths)
# 
#       mel = spec_to_mel_torch(
#           spec,
#           hps.data.filter_length,
#           hps.data.n_mel_channels,
#           hps.data.sampling_rate,
#           hps.data.mel_fmin,
#           hps.data.mel_fmax)
#       y_mel = commons.slice_segments(mel, ids_slice, hps.train.segment_size // hps.data.hop_length)
#       y_hat_mel = mel_spectrogram_torch(
#           y_hat.squeeze(1),
#           hps.data.filter_length,
#           hps.data.n_mel_channels,
#           hps.data.sampling_rate,
#           hps.data.hop_length,
#           hps.data.win_length,
#           hps.data.mel_fmin,
#           hps.data.mel_fmax
#       )
# 
#       y = commons.slice_segments(y, ids_slice * hps.data.hop_length, hps.train.segment_size) # slice
# 
#       # Discriminator
#       y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())
#       with amp.autocast('cuda', enabled=False):
#         loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(y_d_hat_r, y_d_hat_g)
#         loss_disc_all = loss_disc
#     optim_d.zero_grad()
#     scaler.scale(loss_disc_all).backward()
#     scaler.unscale_(optim_d)
#     grad_norm_d = commons.clip_grad_value_(net_d.parameters(), None)
#     scaler.step(optim_d)
# 
#     with amp.autocast('cuda', enabled=hps.train.fp16_run):
#       # Generator
#       y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat)
#       with amp.autocast('cuda', enabled=False):
#         loss_dur = torch.sum(l_length.float())
#         loss_mel = F.l1_loss(y_mel, y_hat_mel) * hps.train.c_mel
#         loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * hps.train.c_kl
# 
#         loss_fm = feature_loss(fmap_r, fmap_g)
#         loss_gen, losses_gen = generator_loss(y_d_hat_g)
#         loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl
#     optim_g.zero_grad()
#     scaler.scale(loss_gen_all).backward()
#     scaler.unscale_(optim_g)
#     grad_norm_g = commons.clip_grad_value_(net_g.parameters(), None)
#     scaler.step(optim_g)
#     scaler.update()
# 
#     if rank==0:
#       if global_step % hps.train.log_interval == 0:
#         lr = optim_g.param_groups[0]['lr']
#         losses = [loss_disc, loss_gen, loss_fm, loss_mel, loss_dur, loss_kl]
#         logger.info('Train Epoch: {} [{:.0f}%]'.format(
#           epoch,
#           100. * batch_idx / len(train_loader)))
#         logger.info([x.item() for x in losses] + [global_step, lr])
# 
#         scalar_dict = {"loss/g/total": loss_gen_all, "loss/d/total": loss_disc_all, "learning_rate": lr, "grad_norm_d": grad_norm_d, "grad_norm_g": grad_norm_g}
#         scalar_dict.update({"loss/g/fm": loss_fm, "loss/g/mel": loss_mel, "loss/g/dur": loss_dur, "loss/g/kl": loss_kl})
# 
#         scalar_dict.update({"loss/g/{}".format(i): v for i, v in enumerate(losses_gen)})
#         scalar_dict.update({"loss/d_r/{}".format(i): v for i, v in enumerate(losses_disc_r)})
#         scalar_dict.update({"loss/d_g/{}".format(i): v for i, v in enumerate(losses_disc_g)})
#         image_dict = {
#             "slice/mel_org": utils.plot_spectrogram_to_numpy(y_mel[0].data.cpu().numpy()),
#             "slice/mel_gen": utils.plot_spectrogram_to_numpy(y_hat_mel[0].data.cpu().numpy()),
#             "all/mel": utils.plot_spectrogram_to_numpy(mel[0].data.cpu().numpy()),
#             "all/attn": utils.plot_alignment_to_numpy(attn[0,0].data.cpu().numpy())
#         }
#         utils.summarize(
#           writer=writer,
#           global_step=global_step,
#           images=image_dict,
#           scalars=scalar_dict)
# 
#       if global_step % hps.train.eval_interval == 0:
#         evaluate(hps, net_g, eval_loader, writer_eval)
#         utils.save_checkpoint(net_g, optim_g, hps.train.learning_rate, epoch, os.path.join(hps.model_dir, "G_{}.pth".format(global_step)))
#         utils.save_checkpoint(net_d, optim_d, hps.train.learning_rate, epoch, os.path.join(hps.model_dir, "D_{}.pth".format(global_step)))
#     global_step += 1
# 
#   if rank == 0:
#     logger.info('====> Epoch: {}'.format(epoch))
# 
# 
# def evaluate(hps, generator, eval_loader, writer_eval):
#     generator.eval()
#     with torch.no_grad():
#       for batch_idx, (x, x_lengths, spec, spec_lengths, y, y_lengths) in enumerate(eval_loader):
#         x, x_lengths = x.cuda(0), x_lengths.cuda(0)
#         spec, spec_lengths = spec.cuda(0), spec_lengths.cuda(0)
#         y, y_lengths = y.cuda(0), y_lengths.cuda(0)
# 
#         # remove else
#         x = x[:1]
#         x_lengths = x_lengths[:1]
#         spec = spec[:1]
#         spec_lengths = spec_lengths[:1]
#         y = y[:1]
#         y_lengths = y_lengths[:1]
#         break
#       # debug
#       print("[EVAL] x", x.shape, "x_lengths", x_lengths, "spec", spec.shape, "spec_lengths", spec_lengths)
#       print("[EVAL] freq_bins_expected", hps.data.filter_length // 2 + 1)
#       y_hat, attn, mask, *_ = generator.module.infer(x, x_lengths, max_len=1000)
#       y_hat_lengths = mask.sum([1,2]).long() * hps.data.hop_length
# 
#       mel = spec_to_mel_torch(
#         spec,
#         hps.data.filter_length,
#         hps.data.n_mel_channels,
#         hps.data.sampling_rate,
#         hps.data.mel_fmin,
#         hps.data.mel_fmax)
#       y_hat_mel = mel_spectrogram_torch(
#         y_hat.squeeze(1).float(),
#         hps.data.filter_length,
#         hps.data.n_mel_channels,
#         hps.data.sampling_rate,
#         hps.data.hop_length,
#         hps.data.win_length,
#         hps.data.mel_fmin,
#         hps.data.mel_fmax
#       )
#     image_dict = {
#       "gen/mel": utils.plot_spectrogram_to_numpy(y_hat_mel[0].cpu().numpy())
#     }
#     audio_dict = {
#       "gen/audio": y_hat[0,:,:y_hat_lengths[0]]
#     }
#     if global_step == 0:
#       image_dict.update({"gt/mel": utils.plot_spectrogram_to_numpy(mel[0].cpu().numpy())})
#       audio_dict.update({"gt/audio": y[0,:,:y_lengths[0]]})
# 
#     utils.summarize(
#       writer=writer_eval,
#       global_step=global_step,
#       images=image_dict,
#       audios=audio_dict,
#       audio_sampling_rate=hps.data.sampling_rate
#     )
#     generator.train()
# 
# 
# if __name__ == "__main__":
#   main()
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/vits/data_utils.py
# 
# import time
# import os
# import random
# import numpy as np
# import torch
# import torch.utils.data
# 
# import commons
# from mel_processing import spectrogram_torch
# from utils import load_wav_to_torch, load_filepaths_and_text
# from text import text_to_sequence, cleaned_text_to_sequence
# 
# 
# class TextAudioLoader(torch.utils.data.Dataset):
#     """
#         1) loads audio, text pairs
#         2) normalizes text and converts them to sequences of integers
#         3) computes spectrograms from audio files.
#     """
#     def __init__(self, audiopaths_and_text, hparams):
#         self.audiopaths_and_text = load_filepaths_and_text(audiopaths_and_text)
#         self.text_cleaners  = hparams.text_cleaners
#         self.max_wav_value  = hparams.max_wav_value
#         self.sampling_rate  = hparams.sampling_rate
#         self.filter_length  = hparams.filter_length
#         self.hop_length     = hparams.hop_length
#         self.win_length     = hparams.win_length
#         self.sampling_rate  = hparams.sampling_rate
# 
#         self.cleaned_text = getattr(hparams, "cleaned_text", False)
# 
#         self.add_blank = hparams.add_blank
#         self.min_text_len = getattr(hparams, "min_text_len", 1)
#         self.max_text_len = getattr(hparams, "max_text_len", 190)
# 
#         random.seed(1234)
#         random.shuffle(self.audiopaths_and_text)
#         self._filter()
# 
# 
#     def _filter(self):
#         """
#         Filter text & store spec lengths
#         """
#         # Store spectrogram lengths for Bucketing
#         # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)
#         # spec_length = wav_length // hop_length
# 
#         audiopaths_and_text_new = []
#         lengths = []
#         for audiopath, text in self.audiopaths_and_text:
#             if self.min_text_len <= len(text) and len(text) <= self.max_text_len:
#                 audiopaths_and_text_new.append([audiopath, text])
#                 lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))
#         self.audiopaths_and_text = audiopaths_and_text_new
#         self.lengths = lengths
# 
#     def get_audio_text_pair(self, audiopath_and_text):
#         # separate filename and text
#         audiopath, text = audiopath_and_text[0], audiopath_and_text[1]
#         text = self.get_text(text)
#         spec, wav = self.get_audio(audiopath)
#         return (text, spec, wav)
# 
#     def get_audio(self, filename):
#         audio, sampling_rate = load_wav_to_torch(filename)
#         if sampling_rate != self.sampling_rate:
#             raise ValueError("{} {} SR doesn't match target {} SR".format(
#                 sampling_rate, self.sampling_rate))
#         audio_norm = audio / self.max_wav_value
#         audio_norm = audio_norm.unsqueeze(0)
#         spec_filename = filename.replace(".wav", ".spec.pt")
# 
#         if os.path.exists(spec_filename):
#             spec = torch.load(spec_filename)
#             print(f"[CACHED SPEC] Loaded from: {spec_filename}")
#             print(f"[CACHED SPEC] Shape after loading: {spec.shape}")
#         else:
#             print(f"[NEW SPEC] Computing for: {filename}")
#             spec = spectrogram_torch(audio_norm, self.filter_length,
#                 self.sampling_rate, self.hop_length, self.win_length,
#                 center=False)
#             print(f"[NEW SPEC] Shape from spectrogram_torch: {spec.shape}")
# 
#             if torch.is_complex(spec):
#                 spec = torch.abs(spec)
#             if spec.dim() == 3:
#                 spec = spec.squeeze(0)
# 
#             torch.save(spec, spec_filename)
# 
#         # ALWAYS normalize shape for both cached and new specs
#         print(f"[NORMALIZE] Before: {spec.shape}")
#         if torch.is_complex(spec):
#             spec = torch.abs(spec)
#         if spec.dim() == 3:
#             spec = spec.squeeze(0)
#             print(f"[NORMALIZE] After squeeze: {spec.shape}")
#         elif spec.dim() == 1:
#             spec = spec.unsqueeze(0)
# 
#         # Verify
#         expected_freq_bins = self.filter_length // 2 + 1
#         print(f"[FINAL] Shape: {spec.shape}, Expected freq bins: {expected_freq_bins}")
#         assert spec.dim() == 2, f"Expected 2D spectrogram, got shape {spec.shape}"
#         assert spec.size(0) == expected_freq_bins, f"Expected {expected_freq_bins} freq bins, got {spec.size(0)}"
# 
#         return spec, audio_norm
# 
# 
#     def get_text(self, text):
#         if self.cleaned_text:
#             text_norm = cleaned_text_to_sequence(text)
#         else:
#             text_norm = text_to_sequence(text, self.text_cleaners)
#         if self.add_blank:
#             text_norm = commons.intersperse(text_norm, 0)
#         text_norm = torch.LongTensor(text_norm)
#         return text_norm
# 
#     def __getitem__(self, index):
#         return self.get_audio_text_pair(self.audiopaths_and_text[index])
# 
#     def __len__(self):
#         return len(self.audiopaths_and_text)
# 
# 
# class TextAudioCollate():
#     """ Zero-pads model inputs and targets
#     """
#     def __init__(self, return_ids=False):
#         self.return_ids = return_ids
# 
#     def __call__(self, batch):
#         """Collate's training batch from normalized text and aduio
#         PARAMS
#         ------
#         batch: [text_normalized, spec_normalized, wav_normalized]
#         """
#         # Right zero-pad all one-hot text sequences to max input length
#         _, ids_sorted_decreasing = torch.sort(
#             torch.LongTensor([x[1].size(1) for x in batch]),
#             dim=0, descending=True)
# 
#         max_text_len = max([len(x[0]) for x in batch])
#         max_spec_len = max([x[1].size(1) for x in batch])
#         max_wav_len = max([x[2].size(1) for x in batch])
# 
#         text_lengths = torch.LongTensor(len(batch))
#         spec_lengths = torch.LongTensor(len(batch))
#         wav_lengths = torch.LongTensor(len(batch))
# 
#         text_padded = torch.LongTensor(len(batch), max_text_len)
#         spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)
#         wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)
#         text_padded.zero_()
#         spec_padded.zero_()
#         wav_padded.zero_()
#         for i in range(len(ids_sorted_decreasing)):
#             row = batch[ids_sorted_decreasing[i]]
# 
#             text = row[0]
#             text_padded[i, :text.size(0)] = text
#             text_lengths[i] = text.size(0)
# 
#             spec = row[1]
#             spec_padded[i, :, :spec.size(1)] = spec
#             spec_lengths[i] = spec.size(1)
# 
#             wav = row[2]
#             wav_padded[i, :, :wav.size(1)] = wav
#             wav_lengths[i] = wav.size(1)
# 
#         if self.return_ids:
#             return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, ids_sorted_decreasing
#         return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths
# 
# 
# """Multi speaker version"""
# class TextAudioSpeakerLoader(torch.utils.data.Dataset):
#     """
#         1) loads audio, speaker_id, text pairs
#         2) normalizes text and converts them to sequences of integers
#         3) computes spectrograms from audio files.
#     """
#     def __init__(self, audiopaths_sid_text, hparams):
#         self.audiopaths_sid_text = load_filepaths_and_text(audiopaths_sid_text)
#         self.text_cleaners = hparams.text_cleaners
#         self.max_wav_value = hparams.max_wav_value
#         self.sampling_rate = hparams.sampling_rate
#         self.filter_length  = hparams.filter_length
#         self.hop_length     = hparams.hop_length
#         self.win_length     = hparams.win_length
#         self.sampling_rate  = hparams.sampling_rate
# 
#         self.cleaned_text = getattr(hparams, "cleaned_text", False)
# 
#         self.add_blank = hparams.add_blank
#         self.min_text_len = getattr(hparams, "min_text_len", 1)
#         self.max_text_len = getattr(hparams, "max_text_len", 190)
# 
#         random.seed(1234)
#         random.shuffle(self.audiopaths_sid_text)
#         self._filter()
# 
#     def _filter(self):
#         """
#         Filter text & store spec lengths
#         """
#         # Store spectrogram lengths for Bucketing
#         # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)
#         # spec_length = wav_length // hop_length
# 
#         audiopaths_sid_text_new = []
#         lengths = []
#         for audiopath, sid, text in self.audiopaths_sid_text:
#             if self.min_text_len <= len(text) and len(text) <= self.max_text_len:
#                 audiopaths_sid_text_new.append([audiopath, sid, text])
#                 lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))
#         self.audiopaths_sid_text = audiopaths_sid_text_new
#         self.lengths = lengths
# 
#     def get_audio_text_speaker_pair(self, audiopath_sid_text):
#         # separate filename, speaker_id and text
#         audiopath, sid, text = audiopath_sid_text[0], audiopath_sid_text[1], audiopath_sid_text[2]
#         text = self.get_text(text)
#         spec, wav = self.get_audio(audiopath)
#         sid = self.get_sid(sid)
#         return (text, spec, wav, sid)
# 
#     def get_audio(self, filename):
#         audio, sampling_rate = load_wav_to_torch(filename)
#         if sampling_rate != self.sampling_rate:
#             raise ValueError("{} {} SR doesn't match target {} SR".format(
#                 sampling_rate, self.sampling_rate))
#         audio_norm = audio / self.max_wav_value
#         audio_norm = audio_norm.unsqueeze(0)
#         spec_filename = filename.replace(".wav", ".spec.pt")
# 
#         if os.path.exists(spec_filename):
#             spec = torch.load(spec_filename)
#             print(f"[CACHED SPEC] Loaded from: {spec_filename}")
#             print(f"[CACHED SPEC] Shape after loading: {spec.shape}")
#             print(f"[CACHED SPEC] Is complex: {torch.is_complex(spec)}")
#         else:
#             print(f"[NEW SPEC] Computing for: {filename}")
#             spec = spectrogram_torch(audio_norm, self.filter_length,
#                 self.sampling_rate, self.hop_length, self.win_length,
#                 center=False)
#             print(f"[NEW SPEC] Shape from spectrogram_torch: {spec.shape}")
#             print(f"[NEW SPEC] Is complex: {torch.is_complex(spec)}")
# 
#             # Handle complex spectrograms
#             if torch.is_complex(spec):
#                 spec = torch.abs(spec)
#                 print(f"[NEW SPEC] Shape after abs(): {spec.shape}")
# 
#             # Remove batch dimension from spectrogram_torch output
#             if spec.dim() == 3:
#                 spec = spec.squeeze(0)
#                 print(f"[NEW SPEC] Shape after squeeze(0): {spec.shape}")
# 
#             torch.save(spec, spec_filename)
#             print(f"[NEW SPEC] Saved to: {spec_filename}")
# 
#         # CRITICAL FIX: Ensure spec is 2D [freq_bins, time_steps]
#         # This applies to BOTH newly computed AND cached spectrograms
#         print(f"[NORMALIZE] Before normalization: {spec.shape}")
# 
#         if torch.is_complex(spec):
#             spec = torch.abs(spec)
#             print(f"[NORMALIZE] After abs(): {spec.shape}")
# 
#         # spec should be [freq_bins, time_steps], NOT [1, freq_bins, time_steps]
#         if spec.dim() == 3:
#             spec = spec.squeeze(0)
#             print(f"[NORMALIZE] After squeeze(0) from 3D: {spec.shape}")
#         elif spec.dim() == 1:
#             spec = spec.unsqueeze(0)
#             print(f"[NORMALIZE] After unsqueeze(0) from 1D: {spec.shape}")
# 
#         print(f"[FINAL] Shape: {spec.shape}")
# 
#         # Verify correct shape
#         assert spec.dim() == 2, f"Expected 2D spectrogram, got shape {spec.shape}"
#         expected_freq_bins = self.filter_length // 2 + 1
#         assert spec.size(0) == expected_freq_bins, f"Expected {expected_freq_bins} freq bins, got {spec.size(0)}"
#         print(f"[FINAL] Verification passed: {expected_freq_bins} freq bins, {spec.size(1)} time steps")
#         print("-" * 80)
# 
#         return spec, audio_norm
# 
#     def get_text(self, text):
#         if self.cleaned_text:
#             text_norm = cleaned_text_to_sequence(text)
#         else:
#             text_norm = text_to_sequence(text, self.text_cleaners)
#         if self.add_blank:
#             text_norm = commons.intersperse(text_norm, 0)
#         text_norm = torch.LongTensor(text_norm)
#         return text_norm
# 
#     def get_sid(self, sid):
#         sid = torch.LongTensor([int(sid)])
#         return sid
# 
#     def __getitem__(self, index):
#         return self.get_audio_text_speaker_pair(self.audiopaths_sid_text[index])
# 
#     def __len__(self):
#         return len(self.audiopaths_sid_text)
# 
# 
# class TextAudioSpeakerCollate():
#     """ Zero-pads model inputs and targets
#     """
#     def __init__(self, return_ids=False):
#         self.return_ids = return_ids
# 
#     def __call__(self, batch):
#         """Collate's training batch from normalized text, audio and speaker identities
#         PARAMS
#         ------
#         batch: [text_normalized, spec_normalized, wav_normalized, sid]
#         """
#         # Right zero-pad all one-hot text sequences to max input length
#         _, ids_sorted_decreasing = torch.sort(
#             torch.LongTensor([x[1].size(1) for x in batch]),
#             dim=0, descending=True)
# 
#         max_text_len = max([len(x[0]) for x in batch])
#         max_spec_len = max([x[1].size(1) for x in batch])
#         max_wav_len = max([x[2].size(1) for x in batch])
# 
#         text_lengths = torch.LongTensor(len(batch))
#         spec_lengths = torch.LongTensor(len(batch))
#         wav_lengths = torch.LongTensor(len(batch))
#         sid = torch.LongTensor(len(batch))
# 
#         text_padded = torch.LongTensor(len(batch), max_text_len)
#         spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)
#         wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)
#         text_padded.zero_()
#         spec_padded.zero_()
#         wav_padded.zero_()
#         for i in range(len(ids_sorted_decreasing)):
#             row = batch[ids_sorted_decreasing[i]]
# 
#             text = row[0]
#             text_padded[i, :text.size(0)] = text
#             text_lengths[i] = text.size(0)
# 
#             spec = row[1]
#             spec_padded[i, :, :spec.size(1)] = spec
#             spec_lengths[i] = spec.size(1)
# 
#             wav = row[2]
#             wav_padded[i, :, :wav.size(1)] = wav
#             wav_lengths[i] = wav.size(1)
# 
#             sid[i] = row[3]
# 
#         if self.return_ids:
#             return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, sid, ids_sorted_decreasing
#         return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, sid
# 
# 
# class DistributedBucketSampler(torch.utils.data.distributed.DistributedSampler):
#     """
#     Maintain similar input lengths in a batch.
#     Length groups are specified by boundaries.
#     Ex) boundaries = [b1, b2, b3] -> any batch is included either {x | b1 < length(x) <=b2} or {x | b2 < length(x) <= b3}.
# 
#     It removes samples which are not included in the boundaries.
#     Ex) boundaries = [b1, b2, b3] -> any x s.t. length(x) <= b1 or length(x) > b3 are discarded.
#     """
#     def __init__(self, dataset, batch_size, boundaries, num_replicas=None, rank=None, shuffle=True):
#         super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)
#         self.lengths = dataset.lengths
#         self.batch_size = batch_size
#         self.boundaries = boundaries
# 
#         self.buckets, self.num_samples_per_bucket = self._create_buckets()
#         self.total_size = sum(self.num_samples_per_bucket)
#         self.num_samples = self.total_size // self.num_replicas
# 
#     def _create_buckets(self):
#         buckets = [[] for _ in range(len(self.boundaries) - 1)]
#         for i in range(len(self.lengths)):
#             length = self.lengths[i]
#             idx_bucket = self._bisect(length)
#             if idx_bucket != -1:
#                 buckets[idx_bucket].append(i)
# 
#         for i in range(len(buckets) - 1, 0, -1):
#             if len(buckets[i]) == 0:
#                 buckets.pop(i)
#                 self.boundaries.pop(i+1)
# 
#         num_samples_per_bucket = []
#         for i in range(len(buckets)):
#             len_bucket = len(buckets[i])
#             total_batch_size = self.num_replicas * self.batch_size
#             rem = (total_batch_size - (len_bucket % total_batch_size)) % total_batch_size
#             num_samples_per_bucket.append(len_bucket + rem)
#         return buckets, num_samples_per_bucket
# 
#     def __iter__(self):
#       # deterministically shuffle based on epoch
#       g = torch.Generator()
#       g.manual_seed(self.epoch)
# 
#       indices = []
#       if self.shuffle:
#           for bucket in self.buckets:
#               indices.append(torch.randperm(len(bucket), generator=g).tolist())
#       else:
#           for bucket in self.buckets:
#               indices.append(list(range(len(bucket))))
# 
#       batches = []
#       for i in range(len(self.buckets)):
#           bucket = self.buckets[i]
#           len_bucket = len(bucket)
#           ids_bucket = indices[i]
#           num_samples_bucket = self.num_samples_per_bucket[i]
# 
#           # add extra samples to make it evenly divisible
#           rem = num_samples_bucket - len_bucket
#           ids_bucket = ids_bucket + ids_bucket * (rem // len_bucket) + ids_bucket[:(rem % len_bucket)]
# 
#           # subsample
#           ids_bucket = ids_bucket[self.rank::self.num_replicas]
# 
#           # batching
#           for j in range(len(ids_bucket) // self.batch_size):
#               batch = [bucket[idx] for idx in ids_bucket[j*self.batch_size:(j+1)*self.batch_size]]
#               batches.append(batch)
# 
#       if self.shuffle:
#           batch_ids = torch.randperm(len(batches), generator=g).tolist()
#           batches = [batches[i] for i in batch_ids]
#       self.batches = batches
# 
#       assert len(self.batches) * self.batch_size == self.num_samples
#       return iter(self.batches)
# 
#     def _bisect(self, x, lo=0, hi=None):
#       if hi is None:
#           hi = len(self.boundaries) - 1
# 
#       if hi > lo:
#           mid = (hi + lo) // 2
#           if self.boundaries[mid] < x and x <= self.boundaries[mid+1]:
#               return mid
#           elif x <= self.boundaries[mid]:
#               return self._bisect(x, lo, mid)
#           else:
#               return self._bisect(x, mid + 1, hi)
#       else:
#           return -1
# 
#     def __len__(self):
#         return self.num_samples // self.batch_size

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/vits/utils.py
# 
# import os
# import glob
# import sys
# import argparse
# import logging
# import json
# import subprocess
# import numpy as np
# from scipy.io.wavfile import read
# import torch
# 
# MATPLOTLIB_FLAG = False
# 
# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
# logger = logging
# 
# 
# def load_checkpoint(checkpoint_path, model, optimizer=None):
#   assert os.path.isfile(checkpoint_path)
#   checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')
#   iteration = checkpoint_dict['iteration']
#   learning_rate = checkpoint_dict['learning_rate']
#   if optimizer is not None:
#     optimizer.load_state_dict(checkpoint_dict['optimizer'])
#   saved_state_dict = checkpoint_dict['model']
#   if hasattr(model, 'module'):
#     state_dict = model.module.state_dict()
#   else:
#     state_dict = model.state_dict()
#   new_state_dict= {}
#   for k, v in state_dict.items():
#     try:
#       new_state_dict[k] = saved_state_dict[k]
#     except:
#       logger.info("%s is not in the checkpoint" % k)
#       new_state_dict[k] = v
#   if hasattr(model, 'module'):
#     model.module.load_state_dict(new_state_dict)
#   else:
#     model.load_state_dict(new_state_dict)
#   logger.info("Loaded checkpoint '{}' (iteration {})" .format(
#     checkpoint_path, iteration))
#   return model, optimizer, learning_rate, iteration
# 
# 
# def save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path):
#   logger.info("Saving model and optimizer state at iteration {} to {}".format(
#     iteration, checkpoint_path))
#   if hasattr(model, 'module'):
#     state_dict = model.module.state_dict()
#   else:
#     state_dict = model.state_dict()
#   torch.save({'model': state_dict,
#               'iteration': iteration,
#               'optimizer': optimizer.state_dict(),
#               'learning_rate': learning_rate}, checkpoint_path)
# 
# 
# def summarize(writer, global_step, scalars={}, histograms={}, images={}, audios={}, audio_sampling_rate=22050):
#   for k, v in scalars.items():
#     writer.add_scalar(k, v, global_step)
#   for k, v in histograms.items():
#     writer.add_histogram(k, v, global_step)
#   for k, v in images.items():
#     writer.add_image(k, v, global_step, dataformats='HWC')
#   for k, v in audios.items():
#     writer.add_audio(k, v, global_step, audio_sampling_rate)
# 
# 
# def latest_checkpoint_path(dir_path, regex="G_*.pth"):
#   f_list = glob.glob(os.path.join(dir_path, regex))
#   f_list.sort(key=lambda f: int("".join(filter(str.isdigit, f))))
#   x = f_list[-1]
#   print(x)
#   return x
# 
# 
# def plot_spectrogram_to_numpy(spectrogram):
#     import matplotlib
#     matplotlib.use('Agg')
#     import matplotlib.pyplot as plt
#     import numpy as np
# 
#     fig, ax = plt.subplots(figsize=(10, 2))
#     im = ax.imshow(spectrogram, aspect="auto", origin="lower",
#                    interpolation='none')
#     plt.colorbar(im, ax=ax)
#     plt.xlabel("Frames")
#     plt.ylabel("Channels")
#     plt.tight_layout()
# 
#     fig.canvas.draw()
# 
#     # FIX: Use buffer_rgba() or tobytes() instead of tostring_rgb()
#     # This works with newer matplotlib versions
#     try:
#         # Try new method first (matplotlib >= 3.0)
#         data = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)
#         data = data.reshape(fig.canvas.get_width_height()[::-1] + (4,))
#         data = data[:, :, :3]  # Remove alpha channel, keep RGB
#     except AttributeError:
#         # Fallback for older matplotlib
#         data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
#         data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))
# 
#     plt.close()
#     return data
# 
# 
# def plot_alignment_to_numpy(alignment, info=None):
#     import matplotlib
#     matplotlib.use('Agg')
#     import matplotlib.pyplot as plt
#     import numpy as np
# 
#     fig, ax = plt.subplots(figsize=(6, 4))
#     im = ax.imshow(alignment.transpose(), aspect='auto', origin='lower',
#                    interpolation='none')
#     fig.colorbar(im, ax=ax)
#     xlabel = 'Decoder timestep'
#     if info is not None:
#         xlabel += '\n\n' + info
#     plt.xlabel(xlabel)
#     plt.ylabel('Encoder timestep')
#     plt.tight_layout()
# 
#     fig.canvas.draw()
# 
#     # FIX: Use buffer_rgba() or tobytes() instead of tostring_rgb()
#     try:
#         # Try new method first (matplotlib >= 3.0)
#         data = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)
#         data = data.reshape(fig.canvas.get_width_height()[::-1] + (4,))
#         data = data[:, :, :3]  # Remove alpha channel, keep RGB
#     except AttributeError:
#         # Fallback for older matplotlib
#         data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
#         data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))
# 
#     plt.close()
#     return data
# 
# 
# def load_wav_to_torch(full_path):
#   sampling_rate, data = read(full_path)
#   return torch.FloatTensor(data.astype(np.float32)), sampling_rate
# 
# 
# def load_filepaths_and_text(filename, split="|"):
#   with open(filename, encoding='utf-8') as f:
#     filepaths_and_text = [line.strip().split(split) for line in f]
#   return filepaths_and_text
# 
# 
# def get_hparams(init=True):
#   parser = argparse.ArgumentParser()
#   parser.add_argument('-c', '--config', type=str, default="./configs/base.json",
#                       help='JSON file for configuration')
#   parser.add_argument('-m', '--model', type=str, required=True,
#                       help='Model name')
# 
#   args = parser.parse_args()
#   model_dir = os.path.join("./logs", args.model)
# 
#   if not os.path.exists(model_dir):
#     os.makedirs(model_dir)
# 
#   config_path = args.config
#   config_save_path = os.path.join(model_dir, "config.json")
#   if init:
#     with open(config_path, "r") as f:
#       data = f.read()
#     with open(config_save_path, "w") as f:
#       f.write(data)
#   else:
#     with open(config_save_path, "r") as f:
#       data = f.read()
#   config = json.loads(data)
# 
#   hparams = HParams(**config)
#   hparams.model_dir = model_dir
#   return hparams
# 
# 
# def get_hparams_from_dir(model_dir):
#   config_save_path = os.path.join(model_dir, "config.json")
#   with open(config_save_path, "r") as f:
#     data = f.read()
#   config = json.loads(data)
# 
#   hparams =HParams(**config)
#   hparams.model_dir = model_dir
#   return hparams
# 
# 
# def get_hparams_from_file(config_path):
#   with open(config_path, "r") as f:
#     data = f.read()
#   config = json.loads(data)
# 
#   hparams =HParams(**config)
#   return hparams
# 
# 
# def check_git_hash(model_dir):
#   source_dir = os.path.dirname(os.path.realpath(__file__))
#   if not os.path.exists(os.path.join(source_dir, ".git")):
#     logger.warn("{} is not a git repository, therefore hash value comparison will be ignored.".format(
#       source_dir
#     ))
#     return
# 
#   cur_hash = subprocess.getoutput("git rev-parse HEAD")
# 
#   path = os.path.join(model_dir, "githash")
#   if os.path.exists(path):
#     saved_hash = open(path).read()
#     if saved_hash != cur_hash:
#       logger.warn("git hash values are different. {}(saved) != {}(current)".format(
#         saved_hash[:8], cur_hash[:8]))
#   else:
#     open(path, "w").write(cur_hash)
# 
# 
# def get_logger(model_dir, filename="train.log"):
#   global logger
#   logger = logging.getLogger(os.path.basename(model_dir))
#   logger.setLevel(logging.DEBUG)
# 
#   formatter = logging.Formatter("%(asctime)s\t%(name)s\t%(levelname)s\t%(message)s")
#   if not os.path.exists(model_dir):
#     os.makedirs(model_dir)
#   h = logging.FileHandler(os.path.join(model_dir, filename))
#   h.setLevel(logging.DEBUG)
#   h.setFormatter(formatter)
#   logger.addHandler(h)
#   return logger
# 
# 
# class HParams():
#   def __init__(self, **kwargs):
#     for k, v in kwargs.items():
#       if type(v) == dict:
#         v = HParams(**v)
#       self[k] = v
# 
#   def keys(self):
#     return self.__dict__.keys()
# 
#   def items(self):
#     return self.__dict__.items()
# 
#   def values(self):
#     return self.__dict__.values()
# 
#   def __len__(self):
#     return len(self.__dict__)
# 
#   def __getitem__(self, key):
#     return getattr(self, key)
# 
#   def __setitem__(self, key, value):
#     return setattr(self, key, value)
# 
#   def __contains__(self, key):
#     return key in self.__dict__
# 
#   def __repr__(self):
#     return self.__dict__.__repr__()

!python /content/vits/preprocess.py \
  --text_index 1 \
  --filelists /content/nepali_tts_dataset/filelists/train_filelist.txt /content/nepali_tts_dataset/filelists/val_filelist.txt \
  --text_cleaners nepali_cleaners

!cat /content/nepali_tts_dataset/filelists/train_filelist.txt.cleaned

# Run these commands in Colab cells:

# 1. Check if core.pyx exists
!ls -la /content/vits/monotonic_align/

# 2. Create the subdirectory that Cython needs
!mkdir -p /content/vits/monotonic_align/monotonic_align

# 3. Now try building again
!cd /content/vits/monotonic_align && python setup.py build_ext --inplace

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/vits

config_content = {
  "train": {
    "log_interval": 200,
    "eval_interval": 1000,
    "seed": 1234,
    "epochs": 10000,
    "learning_rate": 2e-4,
    "betas": [0.8, 0.99],
    "eps": 1e-9,
    "batch_size": 16,
    "fp16_run": True,
    "lr_decay": 0.999875,
    "segment_size": 8192,
    "init_lr_ratio": 1,
    "warmup_epochs": 0,
    "c_mel": 45,
    "c_kl": 1.0
  },
  "data": {
    "training_files": "/content/nepali_tts_dataset/filelists/train_filelist.txt.cleaned",
    "validation_files": "/content/nepali_tts_dataset/filelists/val_filelist.txt.cleaned",
    "text_cleaners": ["nepali_cleaners"],
    "max_wav_value": 32768.0,
    "sampling_rate": 22050,
    "filter_length": 1024,
    "hop_length": 256,
    "win_length": 1024,
    "n_mel_channels": 80,
    "mel_fmin": 0.0,
    "mel_fmax": None,
    "add_blank": True,
    "n_speakers": 0,
    "cleaned_text": True
  },
  "model": {
    "inter_channels": 192,
    "hidden_channels": 192,
    "filter_channels": 768,
    "n_heads": 2,
    "n_layers": 6,
    "kernel_size": 3,
    "p_dropout": 0.1,
    "resblock": "1",
    "resblock_kernel_sizes": [3,7,11],
    "resblock_dilation_sizes": [[1,3,5], [1,3,5], [1,3,5]],
    "upsample_rates": [8,8,2,2],
    "upsample_initial_channel": 512,
    "upsample_kernel_sizes": [16,16,4,4],
    "n_layers_q": 3,
    "use_spectral_norm": False,
    "gin_channels": 0
  }
}

import json
with open("/content/vits/configs/nepali_tts.json", "w") as f:
    json.dump(config_content, f, indent=2)

!sed -i "s/80000/12345/" /content/vits/train.py
!grep MASTER_PORT /content/vits/train.py

# import re

# path = "/content/vits/mel_processing.py"

# with open(path, "r") as f:
#     code = f.read()

# # Replace any torch.stft(...) call that doesn‚Äôt include return_complex
# pattern = r"torch\.stft\(([^)]*?)\)"
# replacement = r"torch.stft(\1, return_complex=True)"
# new_code = re.sub(pattern, replacement, code)

# with open(path, "w") as f:
#     f.write(new_code)

# print("‚úÖ Patched all torch.stft() calls with return_complex=True")

!find /content/nepali_tts_dataset -name "*.spec.pt" -delete
!find /content/vits -name "*.spec.pt" -delete

# Find where your audio files are located
!find /content -name "*.spec.pt" -delete

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/vits
!python train.py -c configs/nepali_tts.json -m nepali_tts